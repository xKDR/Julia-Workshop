{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xKDR/Julia-Workshop/blob/main/MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ1r1bbb0yBv"
      },
      "source": [
        "# <img src=\"https://github.com/JuliaLang/julia-logo-graphics/raw/master/images/julia-logo-color.png\" height=\"100\" /> _Colab Notebook Template_\n",
        "\n",
        "## Instructions\n",
        "1. Work on a copy of this notebook: _File_ > _Save a copy in Drive_ (you will need a Google account). Alternatively, you can download the notebook using _File_ > _Download .ipynb_, then upload it to [Colab](https://colab.research.google.com/).\n",
        "2. If you need a GPU: _Runtime_ > _Change runtime type_ > _Harware accelerator_ = _GPU_.\n",
        "3. Execute the following cell (click on it and press Ctrl+Enter) to install Julia, IJulia and other packages (if needed, update `JULIA_VERSION` and the other parameters). This takes a couple of minutes.\n",
        "4. Reload this page (press Ctrl+R, or ⌘+R, or the F5 key) and continue to the next section.\n",
        "\n",
        "_Notes_:\n",
        "* If your Colab Runtime gets reset (e.g., due to inactivity), repeat steps 2, 3 and 4.\n",
        "* After installation, if you want to change the Julia version or activate/deactivate the GPU, you will need to reset the Runtime: _Runtime_ > _Factory reset runtime_ and repeat steps 3 and 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIeFXS0F0zww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada4f2aa-2969-49d5-805e-71aa430fc27b"
      },
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "#---------------------------------------------------#\n",
        "JULIA_VERSION=\"1.11.2\" # any version ≥ 0.7.0\n",
        "JULIA_PACKAGES=\"IJulia BenchmarkTools\"\n",
        "JULIA_PACKAGES_IF_GPU=\"CUDA\" # or CuArrays for older Julia versions\n",
        "JULIA_NUM_THREADS=2\n",
        "#---------------------------------------------------#\n",
        "\n",
        "if [ -z `which julia` ]; then\n",
        "  # Install Julia\n",
        "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
        "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
        "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
        "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  # Install Packages\n",
        "  nvidia-smi -L &> /dev/null && export GPU=1 || export GPU=0\n",
        "  if [ $GPU -eq 1 ]; then\n",
        "    JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
        "  fi\n",
        "  for PKG in `echo $JULIA_PACKAGES`; do\n",
        "    echo \"Installing Julia package $PKG...\"\n",
        "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
        "  done\n",
        "\n",
        "  # Install kernel and rename it to \"julia\"\n",
        "  echo \"Installing IJulia kernel...\"\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
        "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia\n",
        "\n",
        "  echo ''\n",
        "  echo \"Successfully installed `julia -v`!\"\n",
        "  echo \"Please reload this page (press Ctrl+R, ⌘+R, or the F5 key) then\"\n",
        "  echo \"jump to the 'Checking the Installation' section.\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Julia 1.10.4 on the current Colab Runtime...\n",
            "2024-10-23 05:56:43 URL:https://julialang-s3.julialang.org/bin/linux/x64/1.10/julia-1.10.4-linux-x86_64.tar.gz [173704015/173704015] -> \"/tmp/julia.tar.gz\" [1]\n",
            "Installing Julia package IJulia...\n",
            "Installing Julia package BenchmarkTools...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OS3Ac017T1i"
      },
      "source": [
        "# Checking the Installation\n",
        "The `versioninfo()` function should print your Julia version and some other info about the system:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEzvvzCl1i0F",
        "outputId": "c380c3dc-0d4b-4a54-8726-f54bea852c54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "versioninfo()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Julia Version 1.10.4\n",
            "Commit 48d4fd48430 (2024-06-04 10:41 UTC)\n",
            "Build Info:\n",
            "  Official https://julialang.org/ release\n",
            "Platform Info:\n",
            "  OS: Linux (x86_64-linux-gnu)\n",
            "  CPU: 2 × Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "  WORD_SIZE: 64\n",
            "  LIBM: libopenlibm\n",
            "  LLVM: libLLVM-15.0.7 (ORCJIT, haswell)\n",
            "Threads: 2 default, 0 interactive, 1 GC (on 2 virtual cores)\n",
            "Environment:\n",
            "  LD_LIBRARY_PATH = /usr/lib64-nvidia\n",
            "  JULIA_NUM_THREADS = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using BenchmarkTools\n",
        "\n",
        "M = rand(2^11, 2^11)\n",
        "\n",
        "@btime $M * $M;"
      ],
      "metadata": {
        "id": "YjM_qq54lCcs",
        "outputId": "b432beec-b29f-4c57-ec46-ea35970a39e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  422.390 ms (2 allocations: 32.00 MiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XciCcMAJOT3_",
        "outputId": "4133c183-6840-4004-d606-7e15ef6fd617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try\n",
        "    using CUDA\n",
        "catch\n",
        "    println(\"No GPU found.\")\n",
        "else\n",
        "    run(`nvidia-smi`)\n",
        "    # Create a new random matrix directly on the GPU:\n",
        "    M_on_gpu = CUDA.CURAND.rand(2^11, 2^11)\n",
        "    @btime $M_on_gpu * $M_on_gpu; nothing\n",
        "end"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  4 13:19:11 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8              10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "  421.812 ms (2 allocations: 32.00 MiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RC1QNNqk6h1"
      },
      "source": [
        "# Need Help?\n",
        "\n",
        "* Learning: https://julialang.org/learning/\n",
        "* Documentation: https://docs.julialang.org/\n",
        "* Questions & Discussions:\n",
        "  * https://discourse.julialang.org/\n",
        "  * http://julialang.slack.com/\n",
        "  * https://stackoverflow.com/questions/tagged/julia\n",
        "\n",
        "If you ever ask for help or file an issue about Julia, you should generally provide the output of `versioninfo()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UMidUQB03vJ"
      },
      "source": [
        "Add new code cells by clicking the `+ Code` button (or _Insert_ > _Code cell_).\n",
        "\n",
        "Have fun!\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/JuliaLang/julia-logo-graphics/master/images/julia-logo-mask.png\" height=\"100\" />"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using CUDA"
      ],
      "metadata": {
        "id": "9-nZP8FV0SVg",
        "outputId": "47e9a458-d241-49bd-f06b-735e9dfdaaa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mCircular dependency detected. Precompilation will be skipped for:\n",
            "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  SparseArraysExt [85068d23-b5fb-53f1-8204-05c2aba6942f]\n",
            "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  AtomixCUDAExt [13011619-4c7c-5ef0-948f-5fc81565cd05]\n",
            "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  LinearAlgebraExt [66d79d19-2cc4-5b0b-ac7a-b340256d1ecd]\n",
            "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  KernelAbstractions [63c18a36-062a-441e-b654-da1e3ab1ce7c]\n",
            "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]\n",
            "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Pkg.API /usr/local/share/julia/stdlib/v1.10/Pkg/src/API.jl:1239\u001b[39m\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]\n",
            "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule CUDA with build ID ffffffff-ffff-ffff-0000-0088fd3e685f is missing from the cache.\n",
            "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis may mean CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] does not support precompilation but is imported by a module that does.\n",
            "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1948\u001b[39m\n",
            "\u001b[91m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[91m\u001b[1mError: \u001b[22m\u001b[39mError during loading of extension AtomixCUDAExt of Atomix, use `Base.retry_load_extensions()` to retry.\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m  exception =\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[0m1-element ExceptionStack:\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   Declaring __precompile__(false) is not allowed in files that are being precompiled.\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   Stacktrace:\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m     [1] \u001b[0m\u001b[1m_require\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mpkg\u001b[39m::\u001b[0mBase.PkgId, \u001b[90menv\u001b[39m::\u001b[0mNothing\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1999\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m     [2] \u001b[0m\u001b[1m__require_prelocked\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90muuidkey\u001b[39m::\u001b[0mBase.PkgId, \u001b[90menv\u001b[39m::\u001b[0mNothing\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1812\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m     [3] \u001b[0m\u001b[1m#invoke_in_world#3\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:926\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m     [4] \u001b[0m\u001b[1minvoke_in_world\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:923\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m     [5] \u001b[0m\u001b[1m_require_prelocked\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1803\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m     [6] \u001b[0m\u001b[1m_require_prelocked\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1802\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m     [7] \u001b[0m\u001b[1mrun_extension_callbacks\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mextid\u001b[39m::\u001b[0mBase.ExtensionId\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1295\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m     [8] \u001b[0m\u001b[1mrun_extension_callbacks\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mpkgid\u001b[39m::\u001b[0mBase.PkgId\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1330\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m     [9] \u001b[0m\u001b[1mrun_package_callbacks\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mmodkey\u001b[39m::\u001b[0mBase.PkgId\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1164\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [10] \u001b[0m\u001b[1m_tryrequire_from_serialized\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mmodkey\u001b[39m::\u001b[0mBase.PkgId, \u001b[90mbuild_id\u001b[39m::\u001b[0mUInt128\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1451\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [11] \u001b[0m\u001b[1m_tryrequire_from_serialized\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mpkg\u001b[39m::\u001b[0mBase.PkgId, \u001b[90mpath\u001b[39m::\u001b[0mString, \u001b[90mocachepath\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1524\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [12] \u001b[0m\u001b[1m_require\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mpkg\u001b[39m::\u001b[0mBase.PkgId, \u001b[90menv\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1990\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [13] \u001b[0m\u001b[1m__require_prelocked\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90muuidkey\u001b[39m::\u001b[0mBase.PkgId, \u001b[90menv\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1812\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [14] \u001b[0m\u001b[1m#invoke_in_world#3\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:926\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [15] \u001b[0m\u001b[1minvoke_in_world\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:923\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [16] \u001b[0m\u001b[1m_require_prelocked\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90muuidkey\u001b[39m::\u001b[0mBase.PkgId, \u001b[90menv\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1803\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [17] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1790\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [18] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mlock.jl:267\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [19] \u001b[0m\u001b[1m__require\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90minto\u001b[39m::\u001b[0mModule, \u001b[90mmod\u001b[39m::\u001b[0mSymbol\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1753\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [20] \u001b[0m\u001b[1m#invoke_in_world#3\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:926\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [21] \u001b[0m\u001b[1minvoke_in_world\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:923\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [22] \u001b[0m\u001b[1mrequire\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90minto\u001b[39m::\u001b[0mModule, \u001b[90mmod\u001b[39m::\u001b[0mSymbol\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1746\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [23] \u001b[0m\u001b[1minclude\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mmod\u001b[39m::\u001b[0mModule, \u001b[90m_path\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mBase.jl:495\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [24] \u001b[0m\u001b[1minclude\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mx\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[35mCUDA\u001b[39m \u001b[90m~/.julia/packages/CUDA/2kjXI/src/\u001b[39m\u001b[90m\u001b[4mCUDA.jl:1\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [25] top-level scope\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m~/.julia/packages/CUDA/2kjXI/src/\u001b[39m\u001b[90m\u001b[4mCUDA.jl:123\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [26] \u001b[0m\u001b[1minclude\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mBase.jl:495\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [27] \u001b[0m\u001b[1minclude_package_for_output\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mpkg\u001b[39m::\u001b[0mBase.PkgId, \u001b[90minput\u001b[39m::\u001b[0mString, \u001b[90mdepot_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mdl_load_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mload_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mconcrete_deps\u001b[39m::\u001b[0mVector\u001b[90m{Pair{Base.PkgId, UInt128}}\u001b[39m, \u001b[90msource\u001b[39m::\u001b[0mNothing\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:2222\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [28] top-level scope\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m\u001b[4mstdin:3\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [29] \u001b[0m\u001b[1meval\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:385\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [30] \u001b[0m\u001b[1minclude_string\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mmapexpr\u001b[39m::\u001b[0mtypeof(identity), \u001b[90mmod\u001b[39m::\u001b[0mModule, \u001b[90mcode\u001b[39m::\u001b[0mString, \u001b[90mfilename\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:2076\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [31] \u001b[0m\u001b[1minclude_string\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:2086\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [32] \u001b[0m\u001b[1mexec_options\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mopts\u001b[39m::\u001b[0mBase.JLOptions\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mclient.jl:316\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m    [33] \u001b[0m\u001b[1m_start\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[91m\u001b[1m│ \u001b[22m\u001b[39m   \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mclient.jl:552\u001b[24m\u001b[39m\n",
            "\u001b[91m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1301\u001b[39m\n",
            "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mCircular dependency detected. Precompilation will be skipped for:\n",
            "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  SparseArraysExt [85068d23-b5fb-53f1-8204-05c2aba6942f]\n",
            "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  AtomixCUDAExt [13011619-4c7c-5ef0-948f-5fc81565cd05]\n",
            "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  LinearAlgebraExt [66d79d19-2cc4-5b0b-ac7a-b340256d1ecd]\n",
            "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  KernelAbstractions [63c18a36-062a-441e-b654-da1e3ab1ce7c]\n",
            "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]\n",
            "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Pkg.API /usr/local/share/julia/stdlib/v1.10/Pkg/src/API.jl:1239\u001b[39m\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling AtomixCUDAExt [13011619-4c7c-5ef0-948f-5fc81565cd05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CUDA.functional()"
      ],
      "metadata": {
        "id": "gCJPt-3z0Fy5",
        "outputId": "5cdc4481-8562-43a6-9b51-4fdd9af02bc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "true"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tse6s2RW0IEc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}